= Schedule Hazelcast Pods
:description: Kubernetes allows you to constrain Pods so that they run on specific nodes using the node selector, node affinity, taints, and tolerations scheduling methods. Hazelcast Operator uses these methods to explicitly allow or disallow specific assignments. 

{description}

Hazelcast Management Center also supports these methods.

Pods are a group of one or more containers, which have the following:

* Shared storage.
* Shared network resources.
* A specification, known as a pod spec, that defines how to run the containers.

For further information on pods and the Kubernetes scheduling methods, refer to link:https://kubernetes.io/docs/concepts/workloads/pods/[Pods,window=_blank] and link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[Assigining Pods to Nodes,window=_blank] in the Kubernetes documentation.

== Node Selector

The node selector method, `nodeselector`, defines the nodes on which pods run. Kubernetes only schedules the Pod onto nodes that have each of the specified labels. If no node matches the specified labels, the pods are not scheduled. For example, you can use the node selector method when you want to ensure that Hazelcast pods are run on nodes in a specific region only.

In the following example, we use the built-in link:https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesioregion[`topology.kubernetes.io/region`,window=_blank] label in the pod spec to specify that the pod is to run on nodes in the us-west1 region only. 

The configuration is as follows:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-node-selector.yaml[]
----
--

Hazelcast Management Center::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center-node-selector.yaml[]
----
--
====

For further information on `nodeselector`, refer to the link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector[Kubernetes documentation].

== Node Affinity

Node affinity, `nodeAffinity`, also allows you to constrain the nodes on which pods run, but provides more flexibility than can be provided by node selector. 

With node affinity, you can do the following:

* Prevent a pod from being run if it does not meet a defined rule. This is known as a hard requirement.
* Allow pods to be scheduled if none meet the rule. This is known as a soft requirement. 

=== Example Configuration

In the following example, we set the following:

* A hard requirement for Hazelcast pods to run only on nodes that match labels for AMD64 architecture
and the Linux operating system
* A soft requirement for nodes in the us-west1 or us-west2 regions. 

NOTE: We can't use `nodeselector` to achieve this, because of the soft requirement. 

The configuration is as follows:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-node-affinity.yaml[]
----
--

Hazelcast Management Center::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center-node-affinity.yaml[]
----
--
====

For further information on node affinity, refer to the link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity[Kubernetes documentation,window=_blank].

== Pod Affinity and Pod Anti-Affinity

You can specify pod affinity and pod anti-affinity to define the nodes on which Hazelcast instances
run with respect to other pods.

=== Example Configuration

In this example, we require that Hazelcast pods run on the same nodes as the pods of the `app1` deployment to improve
the application's performance due to the co-located pods. We also prefer that Hazelcast pods each run on a different
node, but - as there might only be a small number of nodes - we don't want to block
the scheduler if the number of Hazelcast pods is greater than the number of nodes. 

The configuration is as follows:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-pod-affinity.yaml[]
----
--

Hazelcast Management Center::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center-pod-affinity.yaml[]
----
--
====

For further information on pod affinity and anti-affinity, refer to the link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity[Kubernetes documentation,window=_blank].

== Tolerations

Tolerations, and the corresponding taints, allow a node to repel pods.
Nodes can use taints with a key, value and particular effect, which means that no pod can schedule to the node unless it matches the toleration.

=== Example Configuration

In this example, we add a taint and see how we can use this to control behavior with toleration matching.

First, we add a taint to 'node1' as follows:

[source,shell]
----
kubectl taint nodes node1 forbidden:NoSchedule

kubectl taint nodes node1 key1=value1:NoSchedule
----

This means that pods cannot schedule onto node1 unless it has a matching toleration.

The following pod cannot schedule any Hazelcast pods on `node1`, as it does not match the toleration:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast.yaml[]
----
--

Hazelcast Management Center::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center.yaml[]
----
--
====

To allow this pod to run on `node1`, we must add `effect` to the pod spec so that it matches the toleration as follows:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-pod-tolerations.yaml[]
----
--

Hazelcast Management Center::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center-pod-tolerations.yaml[]
----
--
====

For further information on tolerations and taints, refer to the link:https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[Kubernetes documentation,window=_blank].

== Topology Spread Constraints

Topology spread constraints control how pods are spread across the cluster, for example among regions, zones, nodes, and other user-defined domains. 

=== Example Configuration

In this example, we ensure that Hazelcast pods are spread across all nodes evenly. To do this, we define the following:

* The maximum permitted difference between the number of matching pods in the target topology.
* The minimum number of matching pods in the nodes.

The configuration is as follows:

[tabs]
====
Hazelcast Operator::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-topology-spread-constraints.yaml[]
----
--

====

NOTE: Hazelcast Management Center has only one instance, so there is no need to use topology spread constraints for the ManagementCenter custom resource.

For further information on topology spread constraints, refer to the link:https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/[Kubernetes documentation,window=_blank].
