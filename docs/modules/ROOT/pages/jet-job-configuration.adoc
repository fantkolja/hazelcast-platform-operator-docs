= Run Data Pipelines
:description: With Hazelcast Operator, you can run data pipelines from existing JAR files for data processing. You can use data pipelines for stream or batch processing.

{description}

To create a data pipeline using the JetJob custom resource, you must configure Jet engine in the Hazelcast custom resource as follows: 

* Enable Jet engine. To do this, set `enable` to `true`.
* Allow the upload of Jet job resources. To do this, set `resourceUploadEnabled` to `true`.

For further information on configuring Jet engine, see xref:jet-engine-configuration.adoc[Configure the Jet Engine].

To learn more about data pipelines and Jet engine, refer to xref:hazelcast:pipelines:overview.adoc[About Data Pipelines,window=_blank] in the Platform documentation.

== Configure the JetJob Resource

The JetJob resource is configured using the following fields: 

[cols="20%m,80%a"]
|===
|Field|Description

|name
|The name of the Jet job to create. If this field is empty, the custom resource name is used. 
You cannot update the name after the Jet job has been created.

|hazelcastResourceName
|The name of the Hazelcast resource for which the Jet job configuration is created.

|state
|The job state.
Default is `Running`. Must be set to `Running` when the JetJob object is created for the first time.
For further information on state management, see <<jetjob-state-management,JetJob State Management>>. 

|jarName
|The name of the JAR to run.
The JAR is located in an external bucket, and placed on the member using `bucketConfig`.

|mainClass
|The name of the main class to be run on the submitted job.

|bucketConfig
|The credentials and URI for the JAR file defined in `jarName`.
The attribtes are as follows:

  - `secretName`. The name of the Secret object holding the credentials for your cloud provider.
  - `bucketURI`. The full path to the external bucket. For example, `gs://your-bucket/path/to/jars`.

|remoteURL
|The URL from which the file is downloaded.

|===

For further information on the fields used to configure Jet jobs, see xref:api-ref.adoc#jetjobspec[JetJobSpec] in the Hazelcast Operator API Reference.

== Provide the JAR for Data Pipeline

To run the Data Pipeline, you must provide a JAR file containing the pipeline. 

You can provide the JAR in either of the following ways:

* Download the JAR before the cluster starts by configuring the `jet.bucketConfig`, `jet.remoteURLs`, or `jet.configMaps` in the Hazelcast custom resource. 
+
This approach ensures that the files in the bucket are accessible to the member when the cluster starts.

* Configure `bucketConfig` or `remoteURL` in the JetJob custom resource. 
+
This approach downloads only the JAR specified in `jarName` before starting the data pipeline.

[jetjob-state-management]
== JetJob State Management
Once the job is created, you can use `state` field to manage its lifecycle.

The valid values are as follows:

- `Running`. All jobs must be created with the `Running` state. Either runs the created job, or starts the `Suspended` job.
- `Suspended`. Gracefully suspends the `Running` job.
- `Canceled`. Gracefully stops the `Running` job.
- `Restarted`. Suspends and resumes the `Running` job in one step.

If you delete the `JetJob` resource, the job is forcefully canceled.

== Example Configuration

In this example, we configure the JetJob resource to run the data pipeline for Hazelcast resources on the source Hazelcast cluster from _my-data-pipeline.jar_.

The configuration is as follows:

.Example configuration
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/jet-job-example.yaml[]
----
