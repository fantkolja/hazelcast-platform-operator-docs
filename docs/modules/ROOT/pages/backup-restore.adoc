= Persistence, Backup and Restore
:description: Persistence allows individual members and whole clusters to recover data using persisted map entries, JCache data, and streaming job snapshots on disk. Members can use persisted data to recover from a planned shutdown (including rolling upgrades), a sudden cluster-wide crash, or a single member failure.
:page-enterprise: true

{description}

For further information on Persistence and how it is used in Hazelcast, refer to xref:hazelcast:storage:persistence.adoc[Persisting Data on Disk,window=_blank] in the Platform documentation. For further information on automating the Hazelcast state management to ensure optimal cluster behavior during shutdown and restart, refer to xref:hazelcast:kubernetes:kubernetes-persistence.adoc[Running Hazelcast Enterprise with Persistence under Kubernetes,window=_blank] in the Platform documentation.

You can create backups in either of the following places:

* Locally. Local backups are kept in-volume and never moved anywhere, which means that the data is constrained to the cluster.
* Externally. External backups are moved to buckets that you provide, which allows the data to be used in different clusters. For example, you might want to move data between two clusters.

TIP: Follow the link:https://docs.hazelcast.com/tutorials/hazelcast-platform-operator-external-backup-restore[Restore a Cluster from Cloud Storage with Hazelcast Platform Operator] tutorial if you prefer a working example.

[enabling-persistence]
== Enable Persistence

Hazelcast persistence is configured as follows:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence.yaml[]
----

<1> Cluster recovery policy. For further information on selecting a cluster recovery policy, see <<recovery-policy, Choosing a Cluster Recovery Policy>>.
<2> Requested size of the PersistentVolumeClaim (PVC) for Hazelcast data persistence. You must calculate the total amount of disk space that you will require.
    Remember that the total amount of used disk space might be larger than the size of your in-memory data, depending on the number of local backups you make.
<3> Optional. This is always `hazelcast/platform-operator-agent`. 
    Agent responsible for moving data between storage locations and cleaning up after the move.
    If you enable `persistence`, but do not configure the agent, Hazelcast Operator uses the latest compatible version of agent.

[trigger-backup]
== Trigger Local Backups

Local backups can be triggered using the `HotBackup` custom resource as follows:

--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-local.yaml[]
----
--

Local backups are always kept in volume.

[triggering-external-backups]
== Trigger External Backups

In some cases, keeping the data only in volume and restoring the data to a single Kubernetes cluster is not enough. You can use external storage to support portablility. For example, when you want to move data between two Kubernetes clusters. 

When persistence is enabled, your Hazelcast pod starts with `platform-operator-agent`. Agent uploads backups to the external bucket that you have configured. 

Hazelcast supports the following external storage:

* AWS S3
* GCP Bucket
* Azure Blob

To trigger an external backup, configure a bucket URI and a secret to tell Hazelcast where to store backup data and how to authenticate as follows:

[tabs]
====

AWS::
+
--
The following configuration stores an external backup in AWS S3:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-aws.yaml[]
----

For further information on the AWS Session authentication process, refer to the link:https://docs.aws.amazon.com/sdk-for-go/api/aws/session/[AWS documentation,window=_blank].

You can pass your credentials when creating a Kubernetes Secret using the following command:

[source,bash]
----
kubectl create secret generic <secret-name> \
	--from-literal=region=<region> \
	--from-literal=access-key-id=<access-key-id> \
	--from-literal=secret-access-key=<secret-access-key>
----
--

GCP::
+
--
The following configuration stores an external backup in GCP Bucket:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-gcp.yaml[]
----

For further information on the GCP principal authentication process, refer to link:https://cloud.google.com/docs/authentication/production/[Authentication at Google,window=_blank] in the Google Cloud documentation.

You can pass the file path to your credentials when creating a Kubernetes Secret using the following command:

[source,bash]
----
kubectl create secret generic <secret-name> \
	--from-file=google-credentials-path=<service_account_json_file>
----
--

Azure::
+
--
The following configuration stores an external backup in Azure Blob:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-azure.yaml[]
----

For further information on the Azure Blob authentication process, refer to link:https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?tabs=azure-portal/[Manage storage account access keys,window=_blank] in the Azure documentation.

You can pass your credentials when creating a Kubernetes Secret using the following command:

[source,bash]
----
kubectl create secret generic <secret-name> \
	--from-literal=storage-account=<storage-account> \
	--from-literal=storage-key=<storage-key>
----
--
====
<1> The Bucket URI in which to store backup data.
<2> Name of the secret with credentials for accessing the Bucket URI.

[scheduling-backups]
== Schedule Backups

You can schedule backups using the `schedule` and `hotBackupTemplate` fields of the `CronHotBackup` resource as follows:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/cron-hot-backup.yaml[]
----

For further information on the `CronHotBackup` resource, see xref:api-ref.adoc#cronhotbackupspec[CronHotBackupSpec] in the Hazelcast Operator API Reference.

The `schedule` field takes a valid cron expression. For example, you can configure the following scheduled backups:

[cols="1,1"]
|===
|`schedule` Value|Description

| ```30 10 * * *```
| Take a backup at 10:30 AM every day

| ```0, 0, 1,15,25 * *```
| Take a backup at midnight on the 1st, 15th, and 25th of each month

| ```@monthly```
| Take a backup at midnight on the first day of each month
|===

For a full list of supported expressions, refer to link:https://pkg.go.dev/github.com/robfig/cron/v3@v3.0.1#hdr-CRON_Expression_Format[CRON Expression Format,window=_blank] in the library documentation.

[checking-the-status-of-a-backup]
== Check the Status of a Backup

To check the status of a local backup, use the following command:

[source,bash]
----
kubectl get hotbackup hot-backup
----

The result is similar to the following:

[source,bash]
----
NAME         STATUS
hot-backup   Success
----

[restoring-from-local-backup]
== Restore from Local Backups

You can only use a local backup once to restore a cluster.

To restore a cluster from a local backup, you can choose from the following approaches:

* Reapply the Hazelcast custom resource directly. 
+
This approach gives the cluster access to the PVCs that contain the persisted data. The Hazelcast cluster is restored from the existing `hot-restart` folders.

* Restore from a local backup, which was created using the HotBackup resource. 
+
To use this approach, you must have configured `hotBackupResource Name` in the `restore` section of your Hazelcast custom resource configuration. For the restore to work correctly, the specified `HotBackup` resource must have been created successfully. You can check the status of your local backup as described in <<scheduling-backups,Scheduling Backups>>
+
For this approach, the `platform-operator-agent` container, also known as the sidecar agent and Agent, is
deployed with the Hazelcast container in the same Pod. Agent starts as an init container, which is a specialized container that runs before the Hazelcast container.
+
The configuration is as follows:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-cr-name.yaml[]
----

<1> `HotBackup` resource name used for restore. The backup folder name is taken from the `HotBackup` resource.
<2> Optional. This is always `hazelcast/platform-operator-agent`. 
Agent responsible for restoring data from local storage and cleaning up after the move. 
If you enable `persistence` and do not pass the agent configuration, Hazelcast Operator uses the latest compatible version of agent.

[restoring-from-external-backups]
== Restore from External Backups

To restore a backed-up cluster from an external bucket, you can configure either:

* The URI and secret of the bucket.
* The name of the HotBackup resource that was used to trigger the external backup.

The `platorm-operator-agent` container is deployed with the Hazelcast container in the same Pod. The agent starts as an init container, also known as the sidecar agent and Agent, which is a specialized container that runs before the Hazelcast container.

NOTE: You must have created the secret. If you have not done so, follow the instructions in <<create-secret, Trigger External Backups>> for your external storage provider.

The configuration is as follows:

[tabs]
====

Bucket Configuration::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-external.yaml[]
----
<1> Bucket URI from which backup data is restored.
<2> Name of the secret with credentials for accessing the specified bucket URI.
<3> Optional. This is always `hazelcast/platform-operator-agent`. 
    Agent responsible for restoring data from external storage and cleaning up after the move. 
    If you enable `persistence` and do not pass the agent configuration, Hazelcast Operator uses the latest compatible version of agent.
--

HotBackup resource name::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-cr-name.yaml[]
----

<1> Name of the `HotBackup` resource used for the restore. The bucket URI and secret are taken from the `HotBackup` resource.
<2> Optional. Agent responsible for restoring data from external storage.
    If you provide `restore` under `persistence` and do not pass the agent configuration, Hazelcast Platform Operator
    uses the latest agent version that is compatible with its version.
--

====

== Configuring Persistence

You can configure the following:

* <<drt,Data recovery timeout>>
* <<recovery-policy,Cluster recovery policy>>
* <<configuring-forcepartial-start,Force or Partial Start>>


[drt]
=== Data Recovery Timeout

The data recovery timeout defines the length of time for which the Hazelcast Operator waits for the recovery of your data to complete. If the recovery takes longer than the configured time, it fails.

The data recovery timeout is set in the `dataRecoveryTimeout` field. The timeout is defined in seconds, and must be an integer. This field is used to set the following Hazelcast persistence options:

* `validation-timeout-seconds`
* `data-load-timeout-seconds`

For further information on these options, refer to xref:hazelcast:storage:configuring-persistence.adoc#global-persistence-options[Global Persistence Options,window=_blank] in the Platform documentation.

In this example, we set the `dataRecoveryTimeout` field to 600 seconds, or 10 minutes.

The configuration is as follows:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-datarecoverytimeout.yaml[]
----

[[recovery-policy]]
=== Cluster Recovery Policy

The cluster recovery policy automates the behavior when one or more members cannot rejoin following a cluster-wide restart. 

The policies that can be applied are described in the following table:

[cols="1,1"]
|===
|Policy|Description

| ```FullRecoveryOnly```
| Default. Do not allow partial start of the cluster.
This policy forces the cluster to wait until all members have restarted. If one or more members do not start, a manual force start is required to ensure that all running members delete their persistence stores and generate new universally unique identifiers (UUIDs). 

| ```PartialRecoveryMostRecent```
| Allow partial start with the members that have most recent partition table.

| ```PartialRecoveryMostComplete```
| Allows partial start with the member that has most complete partition table.
|===


[configuring-forcepartial-start]
=== Configure Force/Partial Start

To recover a cluster that has Persistence enabled after a cluster-wide restart, you can do one of the following:

* Force a cluster to delete their persistence stores when one or more members fail to restart. To do this, you must trigger Force Start.
* Force a cluster to start without some members. To do this, you must trigger Partial Start.

[WARNING]
====
* The cluster can lose all persisted data after a force-start.
* The cluster can lose some persistent data after a partial-start.
====

To trigger the cluster recovery action, configure the `startupAction` as follows:

* Force Start:
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-forcestart.yaml[]
----

* Partial Start:
+
Amend the `clusterDataRecoveryPolicy` value in the above example as follows:

** `PartialRecoveryMostRecent`. Use this value when you want to trigger partial start with the members that have most recent partition table.
** `PartialRecoveryMostComplete`. Use this value when you want to trigger partial start with the member that has most complete partition table.
