= Run Highly Available Hazelcast Clusters
:description: High Availability Mode guarantees that no data is lost, even if the Kubernetes node or whole availability zone is down and all related Hazelcast members are terminated.

With Hazelcast Operator, you can use partition groups and High Availability Mode to add resilience to your clusters. This avoids data loss when a Kubernetes node - or even a whole availability zone - is down, and all related Hazelcast members are terminated.

Partition groups let you choose where Hazelcast members store backups of a data partition. You can configure a partition group to backup data in either of the following ways:

* Inside a member in a different availability zone (ZONE_AWARE).
* On a different Kubernetes node (NODE_AWARE). 

For further information on configuring partition groups, refer to xref:hazelcast:clusters:partition-group-configuration.adoc[Partition Group Configuration,window=_blank] in the Platform documentation. 

When using either partition grouping method (ZONE_AWARE or NODE_AWARE) with a Hazelcast cluster that spans multiple availability zones and nodes, you must have an equal number of members in each zone or node. If the number of members in each zone or node are not equal, partitions are unevenly distributed between members.

Use the `highAvailabilityMode` field to specify partition groups and automatically distribute members across availability zones and nodes using the `topologySpreadConstraints` Kubernetes policy.

For further information on topology spread constraints, refer to the link:https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/[Kubernetes documentation,window=_blank].

== Configure High Availability Mode 

High Availability Mode is configured using the following fields:

[cols="20%m,80%a"]
|===
|Field|Description

|`highAvailabilityMode`
|The partition groups and Kubernetes scheduling policy.

Valid values are as follows:

  - `NODE`. Sets the partition-group to `NODE_AWARE` and adds `topologySpreadConstraints` to the StatefulSet workload API object as follows:

[source,yaml]
```
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
          matchLabels:
            app.kubernetes.io/name: hazelcast
            app.kubernetes.io/instance: hazelcast
            app.kubernetes.io/managed-by: hazelcast-platform-operator
```

  - `ZONE`. Sets the partition-group to `ZONE_AWARE` and adds `topologySpreadConstraints` to the StatefulSet workload API object as follows: 
[source,yaml]
```
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
          matchLabels:
            app.kubernetes.io/name: hazelcast
            app.kubernetes.io/instance: hazelcast
            app.kubernetes.io/managed-by: hazelcast-platform-operator
```

|===

=== Example Configuration

In this example, we configure a partition group to backup data inside a member in a different availability zone.

The configuration is as follows:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-high-availability-mode.yaml[]
----
